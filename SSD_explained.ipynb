{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74418404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import (\n",
    "    ssdlite320_mobilenet_v3_large,\n",
    "    SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    ssd300_vgg16,\n",
    "    SSD300_VGG16_Weights\n",
    ")\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.ops import box_iou, box_convert, complete_box_iou\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import CarImageClass\n",
    "\n",
    "from SSD_from_scratch import mySSD\n",
    "from SSD_trainer import SSD_train, plot_losses, collate_detection, ConditionalIoUCrop, load_checkpoint, build_targets\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# just use cpu for this file\n",
    "device = 'cpu'\n",
    "\n",
    "# desktop or laptop\n",
    "machine = 'laptop'\n",
    "\n",
    "# Setup path to data folder\n",
    "if machine == 'laptop':\n",
    "    folder_path = Path(r\"C:\\self-driving-car\\data\")\n",
    "else:\n",
    "    folder_path = Path(r\"C:\\Udacity_car_data\\data\")\n",
    "\n",
    "train_path = folder_path / \"train\"\n",
    "test_path = folder_path / \"test\"\n",
    "train_path_simple = folder_path / \"train_simple\"\n",
    "test_path_simple = folder_path / \"test_simple\"\n",
    "train_path_oo = folder_path / \"train_one_obj\"\n",
    "test_path_oo = folder_path / \"test_one_obj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ff500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2f54a9a",
   "metadata": {},
   "source": [
    "# How does a single shot detector (SSD) model work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e578b85",
   "metadata": {},
   "source": [
    "First, we need to understand the concept of *prior* bounding boxes, or just *priors*.  The model is equipped with a set of predetermined (by model architecture) priors.  In the case of this SSD model, it has 8732.  Priors are static and never change with model training.  Some of the priors are displayed below in red with the ground truth (GT) bounding box in green.\n",
    "\n",
    "![36 priors and a car](figures/priors_3.gif)\n",
    "![19*19*4 priors and a car](figures/priors_19.gif)\n",
    "![38*38*4 priors and a car](figures/priors_38.gif)\n",
    "\n",
    "As we can see, the priors come in various sizes, since the objective is to be able to detect object of various size.  For this image, we can see the best priors for the GT box (according to an intersection over union score).\n",
    "\n",
    "![Priors above IoU threshold](figures/priors_above_threshold.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd3c0b",
   "metadata": {},
   "source": [
    "### Model architecture and outputs\n",
    "The key features of the model are six convolution layers, which feed into a classification head and localization head.  The name \"single shot detector\" was given since the model predicts class and location in one pass.  The image below is from the paper of W. Liu, et al. (see [here](https://link.springer.com/chapter/10.1007/978-3-319-46448-0_2)) which first introducted the SSD model.\n",
    "\n",
    "![SSD model architecture by W. Liu, et al.](figures/SSD_architecture.png)\n",
    "\n",
    "\n",
    "The ouput of the localization head corresponding to 'conv_4_3' in the image above is a tensor of size $(B, 4*4, 38, 38)$.  Let's breakdown the meaning of each dimension of this tensor (from left to right):\n",
    "\n",
    "* B: batch size\n",
    "* 4: number of dimensions in a prior (the form here is $(x_{\\mathrm{min}}, y_{\\mathrm{min}},x_{\\mathrm{max}}, y_{\\mathrm{max}})$)\n",
    "* 4: number of priors per center\n",
    "* 38: feature map height\n",
    "* 38: feature map width\n",
    "\n",
    "Repeating this process for all six of the convolution layers, the output of the localization head is comprised of tensors of size:\n",
    "* $(B, 4*4, 38, 38)$ (from 'conv_4_3')\n",
    "* $(B, 4*6, 19, 19)$ (from 'conv_7')\n",
    "* $(B, 4*6, 10, 10)$ (from 'conv_8_2')\n",
    "* $(B, 4*6, 5, 5)$ (from 'conv_9_2')\n",
    "* $(B, 4*4, 3, 3)$ (from 'conv_10_2')\n",
    "* $(B, 4*4, 1, 1)$ (from 'conv_11_2')\n",
    "\n",
    "Returning to the images of the priors above, the left/middle/right image shows the priors corresponding 'conv_10_2'/'conv_8_2'/'conv_4_3' layers, respectively.\n",
    "\n",
    "Now it is clear where the number 8732 comes from, since \n",
    "\n",
    "\\begin{equation*}\n",
    "    4*38*38 + 6*19*19 + 6*10*10 + 6*5*5 + 4*3*3 + 4*1*1 = 8732.\n",
    "\\end{equation*}\n",
    "\n",
    "The output of the localization head is the concatenation of the six tensors above, which results in a tensor of size $(B, 8732, 4)$.\n",
    "\n",
    "Similarly, the output of the classification head corresponding to 'conv_4_3' is a tensor of size $(B, C*4, 38, 38)$, where $C$ is the number of classes, and all other numbers are the same as before.  Repeating this process for all six of the convolution layers, the output of the classification head is comprised of tensors of size:\n",
    "* $(B, C*4, 38, 38)$ (from 'conv_4_3')\n",
    "* $(B, C*6, 19, 19)$ (from 'conv_7')\n",
    "* $(B, C*6, 10, 10)$ (from 'conv_8_2')\n",
    "* $(B, C*6, 5, 5)$ (from 'conv_9_2')\n",
    "* $(B, C*4, 3, 3)$ (from 'conv_10_2')\n",
    "* $(B, C*4, 1, 1)$ (from 'conv_11_2')\n",
    "\n",
    "The output of the classification head is the concatenation of the six tensors above, which results in a tensor of size $(B, 8732, C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41597fd2",
   "metadata": {},
   "source": [
    "### How to interpret the model outputs?\n",
    "\n",
    "The output of the classification head is easier to understand.  Given an image (i.e. batch size $B=1$), the classification head outputs are class logits for each prior.  Given a particular prior, the classification head output is $(\\ell_1, \\ell_2, \\ldots, \\ell_{C})$, where $C$ is the number of classes and $\\ell_j$, $j=1,\\ldots, C$, is the logit for class $j$.  The class probabilities are $(p_1, p_2, \\ldots, p_{C})$ are computed via the softmax function, i.e.\n",
    "\\begin{equation*}\n",
    "    p_{j} = \\frac{e^{\\ell_{j}}}{\\sum_{i=1}^{C}e^{\\ell_{i}}}, \\quad j=1,2,\\ldots,C.\n",
    "\\end{equation*}\n",
    "\n",
    "If the classification score corresponding to a particular prior $p = (c_{x}^{p}, c_{y}^{p}, w^{p}, h^{p})$ is high, that means a GT box $g = (c_{x}^{g}, c_{y}^{g}, w^{g}, h^{g})$ should be 'close' to $p$.  The localization head does not predict locations of objects, it predicts offsets to priors of the form $(t_x, t_y, t_w, t_h)$, where\n",
    "\\begin{equation*}\n",
    "t_{x} = \\frac{c_{x}^{\\hat{g}} - c_{x}^{p}}{w^{p}v_{c}}, \\quad t_{y} = \\frac{c_{y}^{\\hat{g}} - c_{y}^{p}}{h^{p}v_{c}}, \\quad t_{w} = \\frac{\\log(w^{\\hat{g}}/w^{p})}{v_{s}}, \\quad t_{h} = \\frac{\\log(h^{\\hat{g}}/h^{p})}{v_{s}},\n",
    "\\end{equation*}\n",
    "where $\\hat{g}=(c_{x}^{\\hat{g}}, c_{y}^{\\hat{g}}, w^{\\hat{g}}, h^{\\hat{g}})$ is the predicted bounding box.  Let us reiterate that the GT box $g$ is unknown and $(t_x, t_y, t_w, t_h)$ are the predicted values, so our predicted bounding box has coordinates \n",
    "\\begin{equation*}\n",
    "c_{x}^{\\hat{g}} = c_{x}^{p} + t_{x}w^{p}v_{c}, \\quad c_{y}^{\\hat{g}} = c_{y}^{p} + t_{y}h^{p}v_{c}, \\quad w^{\\hat{g}} = w^{p}\\mathrm{e}^{t_{w}v_{s}}, \\quad h^{\\hat{g}} = h^{p}\\mathrm{e}^{t_{h}v_{s}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f61ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCPUenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
